#  **ğŸŒŸ My Implementation of Neural Networks Using Hidden Layers ğŸŒŸ**
This project implements a fully connected neural network from scratch using Python and NumPy. It includes forward propagation, backpropagation, and gradient descent for training a model to classify banknotes as genuine or counterfeit. ğŸ’°ğŸ”

## **âœ¨ Features**
âœ… Implements a **customizable** multi-layer neural network.  
âœ… Uses **tanh** activation for hidden layers & **sigmoid** for output.  
âœ… Trains on the **Banknote Authentication Dataset**. ğŸ¦  
âœ… Saves trained weights & biases for later testing. ğŸ§ ğŸ’¾  
âœ… **Custom accuracy formula** based on loss function. ğŸ“Š  

## **ğŸ“‚ Dataset**
The project uses the **Banknote Authentication Dataset** from the UCI Machine Learning Repository. It contains **four extracted features** from banknote images.

ğŸ“Œ **Source:**  [UCI Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt)  

ğŸ“Œ **Features:**

- ğŸ–¼ Variance of wavelet-transformed image

- ğŸ“ˆ Skewness of wavelet-transformed image

- ğŸ“Š Kurtosis of wavelet-transformed image

- ğŸ”¢ Entropy of the image

ğŸ“Œ **Labels:**

- âŒ 0 â†’ Fake banknote

- âœ… 1 â†’ Genuine banknote

## **ğŸ›  Installation & Setup**
### **1ï¸âƒ£ Clone the Repository**
```
git clone https://github.com/MsuhTheGreat/Banknote-NN.git
cd Banknote-NN
```
### **2ï¸âƒ£ Create a Virtual Environment (Optional but Recommended)**
```
python -m venv venv  

# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```
### **3ï¸âƒ£ Install Dependencies**
```
pip install -r requirements.txt
```  
## **ğŸš€ Usage**
### **ğŸ¯ Training the Model**
Run `train.py` to train the neural network:
```
python source/train.py
```
You'll be prompted to enter:  
ğŸ“Œ Number of **epochs** â³  
ğŸ“Œ **Learning rate** (Alpha) ğŸš  
ğŸ“Œ Number of **layers** ğŸ—  
ğŸ“Œ Number of **neurons** in each layer ğŸ”¢  

ğŸ”¹ **Weight Initialization**: Weights are initialized using **Xavier** method.  
ğŸ”¹ **Training Progress**: Cost updates displayed every **1000** epochs.  
ğŸ”¹ **Model Saved To:** `models/` directory. ğŸ’¾  

### **ğŸ§ª Testing the Model**
Run `test.py` to evaluate the trained model:
```
python source/test.py
```  
You'll be asked to enter the number of layers used during training. The script will then:  
âœ… **Perform Forward Propagation** on test data.  
âœ… **Calculate Accuracy** using both traditional formula & **my own custom formula.** ğŸ“Š

#### **Example Output**  
##### **When I used Tanh**  
Accuracy According To My Custom Formula: 99.7289911031016  
Accuracy According To Traditional Formula: 100.0  
##### **When I used ReLU**  
Accuracy According To My Custom Formula: 99.78671729471988  
Accuracy According To Traditional Formula: 100.0  

## **ğŸ“ Project Structure**
```
Banknote-NN/
â”‚â”€â”€ data/                      # Stores test data ğŸ“‚
â”‚â”€â”€ models/                    # Stores trained weights & biases ğŸ§ 
â”‚â”€â”€ source/
â”‚   â”œâ”€â”€ train.py               # Training script ğŸ‹ï¸â€â™‚ï¸
â”‚   â”œâ”€â”€ test.py                # Testing script ğŸ§ª
â”‚â”€â”€ .gitignore                 # Ignore unnecessary files ğŸš«
â”‚â”€â”€ LICENSE                    # MIT License ğŸ“œ
â”‚â”€â”€ README.md                  # Project documentation ğŸ“–
â”‚â”€â”€ Requirements.txt           # Required dependencies ğŸ“¦
```
## **ğŸ¯ Accuracy Calculation**
### **âœ… Traditional Accuracy**
$$
\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}} \times 100
$$  

### **ğŸ”¥ Custom Accuracy Formula**
Uses the sigmoid of the cost function:

$$
J_{\text{sigmoid}} = \frac{1}{1 + e^{-J}}
$$

$$
\text{Accuracy} = (1 - J_{\text{sigmoid}}) \times 200
$$

### **ğŸ”® Intuition for Custom Accuracy Formula**
In Deep Learning, the cost function measures the inaccuracy of a model. I wanted to transform it into a percentage-based accuracy measure. Since the cost function ranges from [0, âˆ), I applied the sigmoid function to map it into a bounded range. Through experimentation, I found that multiplying by 200 effectively scales the accuracy, making it more interpretable.

## **ğŸ¤ Contributing**
If youâ€™d like to contribute:

1. Fork the repo
2. Create a new branch
3. Commit your changes
4. Open a pull request
## **ğŸ“œ License**
This project is open-source and available under the MIT License.
## **ğŸ’¡ Author**
Created by MsuhTheGreat. Feel free to connect! ğŸ˜Š